from test_environment import OthelloEnv  # MÃ´i trÆ°á»ng Othello
from othello import BLACK, WHITE
from q_learning import QNetworkAgent, TrainingMetrics  # Agent dÃ¹ng máº¡ng nÆ¡-ron
from ai.ai_player import RandomPlayer  # Random player
import os
import torch

MODEL_PATH = "models/q_network.pt"

if __name__ == "__main__":
    metrics = TrainingMetrics()
    total_black_win = 0
    total_white_win = 0
    total_draw = 0

    num_games = 1


    # Táº£i model Ä‘Ã£ huáº¥n luyá»‡n
    agent_white = QNetworkAgent()
    if os.path.exists(MODEL_PATH):
        agent_white.model.load_state_dict(torch.load(MODEL_PATH))
        agent_white.model.eval()  # Äáº·t model vá» cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡
        print("âœ… Model loaded from", MODEL_PATH)
    else:
        print("âš ï¸ No saved model found, training from scratch.")

    for game_index in range(num_games):
        env = OthelloEnv()
        random_agent_black = RandomPlayer(env.game)

        observation, info = env.reset()
        done = False
        current_player = BLACK

        total_reward = 0

        while not done:
            if current_player == WHITE:
                # Agent dÃ¹ng máº¡ng nÆ¡-ron
                state = agent_white.encode_state(observation)
                valid_moves = [r * 8 + c for r, c in env.game.get_valid_moves]

                if valid_moves:
                    action = agent_white.choose_action(state, valid_moves)
                    row, col = divmod(action, env.board_size)
                else:
                    action = env.action_space.n - 1  # Pass
                    row, col = None, None
            else:
                move = random_agent_black.play(env.game)
                if move is not None:
                    row, col = move
                    action = row * env.board_size + col
                else:
                    row, col = None, None
                    action = env.action_space.n - 1  # Pass

            # Thá»±c hiá»‡n hÃ nh Ä‘á»™ng
            next_observation, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            next_state = agent_white.encode_state(next_observation)
            next_valid_moves = [r * 8 + c for r, c in env.game.get_valid_moves]

            # Náº¿u WHITE lÃ  ngÆ°á»i vá»«a Ä‘i, thÃ¬ train
            if current_player == WHITE:
                agent_white.train(
                    state=state,
                    action=action,
                    reward=reward,
                    next_state=next_state,
                    done=done,
                    next_valid_moves=next_valid_moves
                )

            observation = next_observation
            current_player = 3 - current_player
            total_reward += reward
            #env.render()

        # Ghi nháº­n káº¿t quáº£ tráº­n Ä‘áº¥u
        # Sau khi káº¿t thÃºc game
        
        winner = env.game.get_winner()
        
        if winner == 2:
            total_white_win += 1
        elif winner == 1:
            total_black_win += 1
        else:
            total_draw += 1

        if agent_white.epsilon > agent_white.epsilon_min:
                    agent_white.epsilon -= agent_white.epsilon_decay

        win_rate = total_white_win / (game_index + 1)
    
        # Sau má»—i game
        win_rate = total_white_win / (game_index + 1)
        metrics.update(
            win_rate=win_rate,
            loss=agent_white.current_loss,
            epsilon=agent_white.epsilon,
            reward=total_reward
        )
        
        # Váº½ biá»ƒu Ä‘á»“ má»—i 50 game
        if game_index % 50 == 0 and game_index > 0:
            metrics.plot()
            print(f"ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ táº¡i training_metrics_{game_index}.png")
            
    # Váº½ biá»ƒu Ä‘á»“ cuá»‘i cÃ¹ng
    metrics.plot("final_training_metrics.png")

    torch.save(agent_white.model.state_dict(), MODEL_PATH)
    print(f"ğŸ’¾ Saved model to {MODEL_PATH}")
    
    # Tá»•ng káº¿t
    print("\n=== Káº¾T QUáº¢ SAU", num_games, "GAME ===")
    print(f"WHITE (QNetworkAgent) tháº¯ng: {total_white_win}")
    print(f"BLACK (RandomPlayer) tháº¯ng: {total_black_win}")
    print(f"HÃ²a: {total_draw}")