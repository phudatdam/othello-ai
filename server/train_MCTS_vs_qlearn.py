from test_environment import OthelloEnv  # M√¥i tr∆∞·ªùng Othello
from othello import BLACK, WHITE
from network.policy_network import PolicyAgent # Agent d√πng m·∫°ng n∆°-ron
from network.metrics import TrainingMetrics
from ai.ai_player import RandomPlayer, MinimaxPlayer  # Random player
import os
import torch
from tqdm import tqdm
BATCH_SIZE=32
MODEL_PATH = "models/MCTS_vs_Mini.pt"
PASS_ACTION = -9 # Gi√° tr·ªã n√†y c·∫ßn kh·ªõp v·ªõi PASS_ACTION_VALUE trong policy_network.py n·∫øu b·∫°n mu·ªën l·ªçc n√≥


if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available else 'cpu')
    print(f'Device: {device}')
    metrics = TrainingMetrics()
    total_black_win = 0
    total_white_win = 0
    total_draw = 0

    num_games = 200

    # T·∫£i model ƒë√£ hu·∫•n luy·ªán
    agent_white = PolicyAgent()
    # Ki·ªÉm tra xem m√¥ h√¨nh PolicyNetwork c√≥ thu·ªôc t√≠nh 'model' kh√¥ng
    # N·∫øu PolicyAgent c·ªßa b·∫°n tr·ª±c ti·∫øp ch·ª©a PolicyNetwork, b·∫°n s·∫Ω truy c·∫≠p n√≥ qua policy_network
    if os.path.exists(MODEL_PATH):
        # Gi·∫£ s·ª≠ agent_white.policy_network l√† ƒë·ªëi t∆∞·ª£ng m·∫°ng th·ª±c s·ª±
        agent_white.policy_network.load_state_dict(torch.load(MODEL_PATH))
        agent_white.policy_network.eval()  # ƒê·∫∑t model v·ªÅ ch·∫ø ƒë·ªô ƒë√°nh gi√°
        print("‚úÖ Model loaded from", MODEL_PATH)
    else:
        print("‚ö†Ô∏è No saved model found, training from scratch.")

    for game_index in tqdm(range(num_games)):
        env = OthelloEnv()
        random_agent_black = MinimaxPlayer(BLACK) # S·ª≠ d·ª•ng RandomPlayer t·ª´ ai_player.py

        observation, info = env.reset()
        done = False
        current_player = BLACK

        total_reward = 0

        while not done:
            env.render()
            if current_player == WHITE:
                
                # L·∫•y n∆∞·ªõc ƒëi h·ª£p l·ªá d∆∞·ªõi d·∫°ng tuple (row, col)
                valid_moves_tuples = env.game.get_valid_moves
                
                if valid_moves_tuples:
                    # S·ª¨A L·ªñI: Truy·ªÅn ƒë·ªëi t∆∞·ª£ng observation g·ªëc v√†o choose_action
                    action = agent_white.choose_action(observation, valid_moves_tuples) 
                    # Chuy·ªÉn ƒë·ªïi tuple (row, col) th√†nh ch·ªâ s·ªë 1D cho env.step n·∫øu c·∫ßn
                    action_for_env_step = action[0] * env.board_size + action[1]
                else:
                    action_for_env_step = PASS_ACTION  # Pass (n·∫øu b·∫°n ƒë·ªãnh nghƒ©a pass l√† action -9)
                    action = PASS_ACTION # L∆∞u tr·ªØ PASS_ACTION v√†o replay buffer

                # M√£ h√≥a tr·∫°ng th√°i HI·ªÜN T·∫†I ƒë·ªÉ l∆∞u v√†o replay buffer
                state_to_store = agent_white.encode_state(observation)
                
                next_observation, reward, terminated, truncated, info = env.step(action_for_env_step)
                done = terminated or truncated
                
                # M√£ h√≥a tr·∫°ng th√°i TI·∫æP THEO ƒë·ªÉ l∆∞u v√†o replay buffer
                next_state_to_store = agent_white.encode_state(next_observation)
                
                # L∆∞u tr·ªØ kinh nghi·ªám v√†o replay buffer
                agent_white.store_experience(
                    state=state_to_store,
                    action=action, # action ·ªü ƒë√¢y c√≥ th·ªÉ l√† tuple (row, col) ho·∫∑c PASS_ACTION
                    reward=reward,
                    next_state=next_state_to_store,
                    done=done
                )
                
                # C·∫≠p nh·∫≠t ch√≠nh s√°ch sau m·ªói l∆∞·ª£t c·ªßa WHITE n·∫øu ƒë·ªß kinh nghi·ªám
                if len(agent_white.memory) > BATCH_SIZE:
                    agent_white.update_policy(BATCH_SIZE)

            else: # L∆∞·ª£t c·ªßa BLACK (RandomPlayer)
                move = random_agent_black.play(env.game) # play() c·ªßa RandomPlayer tr·∫£ v·ªÅ (row, col) ho·∫∑c None
                if move is not None:
                    row, col = move
                    action_for_env_step = row * env.board_size + col
                else:
                    action_for_env_step = PASS_ACTION  # Pass
                
                next_observation, reward, terminated, truncated, info = env.step(action_for_env_step)
                done = terminated or truncated

            observation = next_observation
            current_player = 3 - current_player
            
        total_reward += reward
            #env.render()

        # Ghi nh·∫≠n k·∫øt qu·∫£ tr·∫≠n ƒë·∫•u
        winner = env.game.get_winner()
        
        if winner == WHITE: # WHITE l√† 2
            total_white_win += 1
        elif winner == BLACK: # BLACK l√† 1
            total_black_win += 1
        else: # H√≤a
            total_draw += 1
    
        # C·∫≠p nh·∫≠t metrics sau m·ªói game
        win_rate = total_white_win / (game_index + 1)
        metrics.update(
            win_rate=win_rate,
            loss=agent_white.current_loss, # S·ª¨A L·ªñI: S·ª≠ d·ª•ng current_loss thay v√¨ losses[-1]
            epsilon=agent_white.epsilon,
            reward=total_reward
        )
        
        # V·∫Ω bi·ªÉu ƒë·ªì m·ªói 5 game (ho·∫∑c s·ªë l∆∞·ª£ng b·∫°n mu·ªën)
        if game_index % 10 == 0 and game_index > 0:
            metrics.plot(save_path=f"training_metrics_{game_index}.png") # L∆∞u v·ªõi t√™n kh√°c nhau
            torch.save(agent_white.policy_network.state_dict(), MODEL_PATH)
            print(f"ƒê√£ l∆∞u bi·ªÉu ƒë·ªì t·∫°i training_metrics_{game_index}.png")
            
    # V·∫Ω bi·ªÉu ƒë·ªì cu·ªëi c√πng
    metrics.plot("final_training_metrics.png")

    # L∆∞u model c·ªßa PolicyNetwork
    # Gi·∫£ s·ª≠ agent_white.policy_network l√† ƒë·ªëi t∆∞·ª£ng m·∫°ng th·ª±c s·ª±
    torch.save(agent_white.policy_network.state_dict(), MODEL_PATH)
    print(f"üíæ Saved model to {MODEL_PATH}")
    
    # T·ªïng k·∫øt
    print("\n=== K·∫æT QU·∫¢ SAU", num_games, "GAME ===")
    print(f"WHITE (PolicyAgent) th·∫Øng: {total_white_win}")
    print(f"BLACK (RandomPlayer) th·∫Øng: {total_black_win}")
    print(f"H√≤a: {total_draw}")

