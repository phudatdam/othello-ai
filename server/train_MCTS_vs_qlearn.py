from test_environment import OthelloEnv  # MÃ´i trÆ°á»ng Othello
from othello import BLACK, WHITE
from network.policy_network import PolicyAgent # Agent dÃ¹ng máº¡ng nÆ¡-ron
from network.metrics import TrainingMetrics
from ai.ai_player import RandomPlayer, MinimaxPlayer  # Random player
import os
import torch
from tqdm import tqdm
BATCH_SIZE=32
MODEL_PATH = "models/MCTS_vs_q.pt"
PASS_ACTION = -9 # GiÃ¡ trá»‹ nÃ y cáº§n khá»›p vá»›i PASS_ACTION_VALUE trong policy_network.py náº¿u báº¡n muá»‘n lá»c nÃ³

if __name__ == "__main__":
    metrics = TrainingMetrics()
    total_black_win = 0
    total_white_win = 0
    total_draw = 0

    num_games = 2000

    # Táº£i model Ä‘Ã£ huáº¥n luyá»‡n
    agent_white = PolicyAgent()
    # Kiá»ƒm tra xem mÃ´ hÃ¬nh PolicyNetwork cÃ³ thuá»™c tÃ­nh 'model' khÃ´ng
    # Náº¿u PolicyAgent cá»§a báº¡n trá»±c tiáº¿p chá»©a PolicyNetwork, báº¡n sáº½ truy cáº­p nÃ³ qua policy_network
    if os.path.exists(MODEL_PATH):
        # Giáº£ sá»­ agent_white.policy_network lÃ  Ä‘á»‘i tÆ°á»£ng máº¡ng thá»±c sá»±
        agent_white.policy_network.load_state_dict(torch.load(MODEL_PATH))
        agent_white.policy_network.eval()  # Äáº·t model vá» cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡
        print("âœ… Model loaded from", MODEL_PATH)
    else:
        print("âš ï¸ No saved model found, training from scratch.")

    for game_index in tqdm(range(num_games)):
        env = OthelloEnv()
        random_agent_black = RandomPlayer(BLACK) # Sá»­ dá»¥ng RandomPlayer tá»« ai_player.py

        observation, info = env.reset()
        done = False
        current_player = BLACK

        total_reward = 0

        while not done:
            if current_player == WHITE:
                # Láº¥y nÆ°á»›c Ä‘i há»£p lá»‡ dÆ°á»›i dáº¡ng tuple (row, col)
                valid_moves_tuples = env.game.get_valid_moves
                
                if valid_moves_tuples:
                    # Sá»¬A Lá»–I: Truyá»n Ä‘á»‘i tÆ°á»£ng observation gá»‘c vÃ o choose_action
                    action = agent_white.choose_action(observation, valid_moves_tuples) 
                    # Chuyá»ƒn Ä‘á»•i tuple (row, col) thÃ nh chá»‰ sá»‘ 1D cho env.step náº¿u cáº§n
                    action_for_env_step = action[0] * env.board_size + action[1]
                else:
                    action_for_env_step = PASS_ACTION  # Pass (náº¿u báº¡n Ä‘á»‹nh nghÄ©a pass lÃ  action -9)
                    action = PASS_ACTION # LÆ°u trá»¯ PASS_ACTION vÃ o replay buffer

                # MÃ£ hÃ³a tráº¡ng thÃ¡i HIá»†N Táº I Ä‘á»ƒ lÆ°u vÃ o replay buffer
                state_to_store = agent_white.encode_state(observation)
                
                next_observation, reward, terminated, truncated, info = env.step(action_for_env_step)
                done = terminated or truncated
                
                # MÃ£ hÃ³a tráº¡ng thÃ¡i TIáº¾P THEO Ä‘á»ƒ lÆ°u vÃ o replay buffer
                next_state_to_store = agent_white.encode_state(next_observation)
                
                # LÆ°u trá»¯ kinh nghiá»‡m vÃ o replay buffer
                agent_white.store_experience(
                    state=state_to_store,
                    action=action, # action á»Ÿ Ä‘Ã¢y cÃ³ thá»ƒ lÃ  tuple (row, col) hoáº·c PASS_ACTION
                    reward=reward,
                    next_state=next_state_to_store,
                    done=done
                )
                
                # Cáº­p nháº­t chÃ­nh sÃ¡ch sau má»—i lÆ°á»£t cá»§a WHITE náº¿u Ä‘á»§ kinh nghiá»‡m
                if len(agent_white.memory) > BATCH_SIZE:
                    agent_white.update_policy(BATCH_SIZE)

            else: # LÆ°á»£t cá»§a BLACK (RandomPlayer)
                move = random_agent_black.play(env.game) # play() cá»§a RandomPlayer tráº£ vá» (row, col) hoáº·c None
                if move is not None:
                    row, col = move
                    action_for_env_step = row * env.board_size + col
                else:
                    action_for_env_step = PASS_ACTION  # Pass
                
                next_observation, reward, terminated, truncated, info = env.step(action_for_env_step)
                done = terminated or truncated

            observation = next_observation
            current_player = 3 - current_player
            
        total_reward += reward
            #env.render()

        # Ghi nháº­n káº¿t quáº£ tráº­n Ä‘áº¥u
        winner = env.game.get_winner()
        
        if winner == WHITE: # WHITE lÃ  2
            total_white_win += 1
        elif winner == BLACK: # BLACK lÃ  1
            total_black_win += 1
        else: # HÃ²a
            total_draw += 1
    
        # Cáº­p nháº­t metrics sau má»—i game
        win_rate = total_white_win / (game_index + 1)
        metrics.update(
            win_rate=win_rate,
            loss=agent_white.current_loss, # Sá»¬A Lá»–I: Sá»­ dá»¥ng current_loss thay vÃ¬ losses[-1]
            epsilon=agent_white.epsilon,
            reward=total_reward
        )
        
        # Váº½ biá»ƒu Ä‘á»“ má»—i 5 game (hoáº·c sá»‘ lÆ°á»£ng báº¡n muá»‘n)
        if game_index % 1000 == 0 and game_index > 0:
            metrics.plot(save_path=f"training_metrics_{game_index}.png") # LÆ°u vá»›i tÃªn khÃ¡c nhau
            print(f"ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ táº¡i training_metrics_{game_index}.png")
            
    # Váº½ biá»ƒu Ä‘á»“ cuá»‘i cÃ¹ng
    metrics.plot("final_training_metrics.png")

    # LÆ°u model cá»§a PolicyNetwork
    # Giáº£ sá»­ agent_white.policy_network lÃ  Ä‘á»‘i tÆ°á»£ng máº¡ng thá»±c sá»±
    torch.save(agent_white.policy_network.state_dict(), MODEL_PATH)
    print(f"ğŸ’¾ Saved model to {MODEL_PATH}")
    
    # Tá»•ng káº¿t
    print("\n=== Káº¾T QUáº¢ SAU", num_games, "GAME ===")
    print(f"WHITE (PolicyAgent) tháº¯ng: {total_white_win}")
    print(f"BLACK (RandomPlayer) tháº¯ng: {total_black_win}")
    print(f"HÃ²a: {total_draw}")

